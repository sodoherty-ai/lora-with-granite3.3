{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T19:20:52.702697Z",
     "start_time": "2025-04-20T19:20:52.631007Z"
    }
   },
   "source": [
    "from sympy.physics.units import temperature\n",
    "\n",
    "model_folder = 'model'\n",
    "base_model_id = 'ibm-granite/granite-3.3-8b-instruct'\n",
    "lora_adapter = f'./{model_folder}/granite3.3-lora-adapter'"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:20:53.027918Z",
     "start_time": "2025-04-20T19:20:53.025457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "a19727112215a9bf",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup model and prompt\n",
    "\n",
    "note: you can ignore the `'NoneType' object has no attribute 'cadam32bit_grad_fp32'` message."
   ],
   "id": "42037d6d0d315ece"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:21:06.372647Z",
     "start_time": "2025-04-20T19:20:53.876853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        device_map='auto',\n",
    "        torch_dtype='auto',\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "    lora_adapter\n",
    ")\n",
    "\n",
    "_ = model.eval()\n"
   ],
   "id": "ca8f42bf0d8501e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e56faf0cb074313ae2add19f7622d61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "239237ad66ae4ee6a0f249691ef466fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:21:10.926037Z",
     "start_time": "2025-04-20T19:21:10.921286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = '''Follow these rules exactly:\n",
    "- Read the question and give your answer in plain English.\n",
    "- **Do not** make anything up.\n",
    "- Keep it brief. Do not elaborate.\n",
    "- Once you have written answer write \"---STOP---\" and nothing further.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "'''\n",
    "\n",
    "def generate(question, use_baseline_mode=False, debug=False):\n",
    "    inputs = tokenizer(prompt.format(question=question), return_tensors='pt').to(model.device)\n",
    "\n",
    "    if use_baseline_mode:\n",
    "        with torch.no_grad():\n",
    "            outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                top_p=0.90,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                top_p=0.90,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if debug:\n",
    "        print(full_response)\n",
    "\n",
    "    response = full_response.replace(prompt.format(question=question), '')\n",
    "    response = response.split('---STOP---')[0]\n",
    "    return response\n",
    "\n",
    "def question(text):\n",
    "    display(Markdown(f'#### Question: `{text}`\\n---'))\n",
    "    display(Markdown('\\n#### LoRA model answer'))\n",
    "    display(Markdown(generate(text)))\n",
    "    display(Markdown('\\n---\\n#### Base model answer'))\n",
    "    display(Markdown(generate(text, use_baseline_mode=True)))\n",
    "\n"
   ],
   "id": "c8cdf1d45a1fdd77",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:29:36.554801Z",
     "start_time": "2025-04-20T19:29:10.408539Z"
    }
   },
   "cell_type": "code",
   "source": "question('What is the \"Trigger Words\" feature in watsonx Assistant?')",
   "id": "902fd085ef083fce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "#### Question: `What is the \"Trigger Words\" feature in watsonx Assistant?`\n---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### LoRA model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The \"Trigger Words\" feature in watsonx Assistant is designed to detect and prevent user inputs from triggering sensitive actions or responses that might be harmful or inappropriate.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n---\n#### Base model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The \"Trigger Words\" feature in watsonx Assistant allows users to specify certain words or phrases that, when present in a user's input, will activate a predefined response or action. This enables the assistant to perform specific tasks or provide particular information in response to cues from the user's input. "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:42:37.493607Z",
     "start_time": "2025-04-20T19:42:17.077430Z"
    }
   },
   "cell_type": "code",
   "source": "question('How do I stop my action being used as a clarifying question?')",
   "id": "e195f221895a653e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "#### Question: `How do I stop my action being used as a clarifying question?`\n---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### LoRA model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "You can stop an action from being used as a clarifying question in the following ways:\n\n- 1.  If you want to stop all actions from being used as clarifying questions, go to Home &gt; Actions &gt; Settings &gt; Clarifying questions and switch the toggle to Off.\n- 2.  If you want to stop a specific action from being used as a clarifying question, open the action and go to Action settings &gt; Clarifying question and switch the toggle to Off.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n---\n#### Base model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "To prevent your action from being interpreted as a clarifying question, be clear and direct in your communication. State your intentions explicitly and avoid ambiguous language. If you're providing information, present it as a statement rather than a query. For instance, instead of saying \"Isn't it true that...?\", say \"The fact is that...\".\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T19:47:09.210078Z",
     "start_time": "2025-04-20T19:46:50.905261Z"
    }
   },
   "cell_type": "code",
   "source": "question('What are the response types available to use?')",
   "id": "7a198999f61cc11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "#### Question: `What are the response types available to use?`\n---"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### LoRA model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The response types available to use are text, image, video, audio, iframe, and options.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n---\n#### Base model answer"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The response types available to use are:\n- Plain text\n- Numbered or bulleted lists\n- Yes/No responses\n- Short phrases or words\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b4109db778c1ad4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
